import json
import requests
from openai import OpenAI
from pydantic import BaseModel, Field


# Set up local model access

client = OpenAI(
    base_url='http://localhost:11434/v1',
    api_key='ollama',  # required but unused
)

# Define the function that we want to call for find_weather

def find_weather(latitude, longitude):
    print(f"\nðŸ“¡ Calling find_weather with:")
    print(f"  latitude: {latitude}")
    print(f"  longitude: {longitude}")

    response = requests.get(
        f"https://api.open-meteo.com/v1/forecast"
        f"?latitude={latitude}"
        f"&longitude={longitude}"
        f"&current=temperature_2m,wind_speed_10m"
        f"&temperature_unit=fahrenheit"
        f"&windspeed_unit=mph"
    )

    print(f"\n Raw API response:")
    print(json.dumps(response.json(), indent=2))

    data = response.json()
    return data["current"]

# Expose the function as a tool for the agent

tools = [
    {
        "type": "function",
        "function": {
            "name": "find_weather",
            "description": "Find current weather for provided coordinates.",
            "parameters": {
                "type": "object",
                "properties": {
                    "latitude": {"type": "number"},
                    "longitude": {"type": "number"},
                },
                "required": ["latitude", "longitude"],
                "additionalProperties": False,
            },
            "strict": True,
        },
    }
]

# System prompt

system_prompt = """You are an AI assistant designed to help users find weather conditions. Your primary goal is to provide precise, helpful, and clear responses.

You have access to the following tools:

Tool Name: find_weather, Description: Get weather for a location., Arguments: latitude: float, longitude: float, Outputs: string

If necessary, call a tool with the proper JSON formatting "Action: {JSON_BLOB}". 

If you call the tool, your final answer **must** include the data returned from the tool call. Do not make up any data. 

Print your final answer starting with the prefix "Final Answer:"
"""

# Get one-time user input

user_input = input("\nAsk your question: ")

# Prepare initial messages

messages = [
    {"role": "system", "content": system_prompt},
    {"role": "user", "content": user_input},
]

# First model call
completion = client.chat.completions.create(
    model="llama3.2",
    messages=messages,
    tools=tools,
)

response_dict = completion.model_dump()
message = response_dict["choices"][0]["message"]
tool_calls = message.get("tool_calls", [])

# Handle tool call if needed

if tool_calls:
    def call_function(name, args):
        if name == "find_weather":
            return find_weather(**args)

    for tool_call in tool_calls:
        name = tool_call["function"]["name"]
        args = json.loads(tool_call["function"]["arguments"])
        messages.append(message)

        result = call_function(name, args)
        messages.append({
            "role": "tool",
            "tool_call_id": tool_call["id"],
            "content": json.dumps(result)
        })

# Final model call to generate complete response

class WeatherResponse(BaseModel):
    response: str = Field(description="A natural language response to the user's question.")

completion_2 = client.beta.chat.completions.parse(
    model="llama3.2",
    messages=messages,
    tools=tools,
    response_format=WeatherResponse,
)

final_response = completion_2.choices[0].message.parsed
print("\nâœ…", final_response.response)
